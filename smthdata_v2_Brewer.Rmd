---
title: "smthdata_v2_Brewer"
author: "Simon Brewer modified by Timothy Terlizzi"
date: "2023-01-23"
output: html_document
---

# Smoother Context
The code within this file is used to apply a weighted mean interpolate method to smooth the raw pollen relative abundances to a uniform temporal resolution. The pollen relative abundances come from 29 different records across North America. The pollen record for each site was accessed via the NEOTOMA Paleoecology Database. The raw pollen abundance data for each site can be accessed in the rawdata folder within the working directory *note the pollen records for each site have already been filtered for only common plant taxa and converted to relative abundances and prior to being imported into this file. If sites are downloaded directly from NEOTOMA, they must be converted into relative abundances rather than counts before the site can be smoothed* The temporal range and resolution of each site varies between sites, therefore we opted to smooth all the sites to the same temporal range and resolution. In this case, we opted to smooth all sites to a 1000-year resolution going from 0 to 150,000 cal yr BP with a window of 10,000 years, however this can be changes to any resolution and range. This code imports the required packages as well as runs the weighted means interpolations for each site using the defined window parameters. The smoothed sites are saved in the smthdata folder within the working directory. The code also merges all the site files into a masterfile that is saved within the composite folder. The master file is used in the other .rmd scripts within this project. *note the age models for each site likely differ from one another. This interpolation does not incorporate the error of each age model for each site, rather the goal is that by smoothing to a resolution of every 1000 years, the error of individual dates is minimized*

# Libraries and Raw Data
### Required Packages
```{r}
require(rioja)
```

### Libraries of packages
```{r}
library(plyr)
```

### Importing raw data
```{r Setup, echo=TRUE}
#importing the raw csv site files containing relative abundances of pollen taxa within sediment cores from rawdata folder
#each site file contains columns for each identified plant taxa as well as a sample name column and a calibrated date column, note the floristic composition of each site is unique, though some sites share common taxa
files = list.files("./rawdata/", glob2rx("*.csv")) 

#establishing window size and temporal range and resolution
halfWin = 5000 #half of the window for the moving window analysis
#create our age range (in this case from 0 to 150,000 years ago by 1000 year intervals)
newyr=seq(0,150000,1000) 
```

# Weighted Mean Interpolation

This code is used to smooth the sites to the desired 1000-year resolution with interpolation points contained within a 10,000-year window. This is accomplished by establishing weights for each sample based on the desired year and the size of the window. Each site is then smoothed by applying the weighted mean interpolation function before saving the smoothed site as a new data frame and writing a .csv file within the smthdata folder of the working directory. 

```{r Smoothing_the_sites, echo=TRUE}
#variables below: poll: raw pollen data from n sites presented in n number of csv files, taxanames: names of pollen taxa in raw dataset, allnames: names of all the columns in the site file, site: the name of the n site, ages: calibrated dates for each sample within n site, agerange: temporal range of site n, nbtaxa: number of taxa in n site

#creating the variables to be used in the for loop based on the first csv within files
poll = read.csv(paste("./rawdata/",files[1],sep='')) #poll is the entire csv of pollen abundances as well as sample names and calibrated dates
taxanames = names(poll)[-c(1,2)] #this is a list of taxa in the site, column 1 and 2 are cut as they are not plant taxa
allnames = names(poll) #this is all of the names of the site, including column 1 and 2

#looping for each site (29 sites)
for (i in 1:length(files)) {
  poll = read.csv(paste("./rawdata/",files[i],sep='')) #store the site rawdata as poll
  sitename = strsplit(files[i],"[.]")[[1]][1] #get the site name
  taxanames = names(poll)[-c(1,2)] #pull the names of all the taxa
  allnames = names(poll) #all of the column names  
  
  site = substr(poll[1,1],0,4) #the 4-letter site name 
  ages = poll[,2] #the dates for the site samples (column 2)
  agerange = diff(range(ages)) #total age range, difference between the youngest and oldest sample
  poll = poll[,-c(1,2)] #leaving just the pollen relative proportions, removing columns 1 and 2 which contain the sample names and the dates
  
  nbtaxa = dim(poll)[2] #number of columns in poll aka the number of taxa
  
  #creating a matrix with the length of the sequence (0-150,000 by 1000) and the same number of columns as taxa
  outmat = matrix(NA, nrow=length(newyr), ncol=nbtaxa) 
  
  #looping for each time step (row)
  for (j in 1:length(newyr)) {
    
    #for each year (0-150,000 by 1000), this determines the range of the window (10,000 years around the sample age) this is the lower limit
    halfWinLo <- newyr[j] - halfWin 
    halfWinHi <- newyr[j] + halfWin #this is the upper limit
    #storing the taxa abundances for the samples within the window
    myPercTS <- poll[(ages <= halfWinHi & ages >= halfWinLo),] 
    #storing the dates that fall within the window
    myAgeTS <- ages[(ages <= halfWinHi & ages >= halfWinLo)] 
  
    if (dim(myPercTS)[1] > 0) { #if there are samples within the window:
      
      #create the weights for each sample by taking the difference between each sample age and the current date and subtracting from the half window
      wts <- halfWin - abs(myAgeTS-newyr[j]) 
      wts <- wts / sum(wts) #finalizing the weights by dividing each by the sum
      
      #The resulting weights are larger for samples falling closer to the window and             smaller for samples farther away from the window
      
      #smoothing the data via the weighted means function and using the calculated wts as the weight argument and storing as the outmat matrix
      outmat[j,] <- apply(myPercTS,2,weighted.mean,w=wts)
     
    }
  }

  #saving a pdf of the raw pollen counts for each site as a strat plot
  #The plots are saved within the stratplots folder
  pdf(paste("./stratplots/",sitename,"_raw.pdf", sep=''))
  strat.plot(poll*100, yvar=ages, y.rev=TRUE, x.names=taxanames, 
             scale.percent=FALSE, cex.xlabel=0.7, cex.yaxis=0.7)
  dev.off()

  #saving a pdf of the smoothed pollen counts for each site as a strat plot
  pdf(paste("./stratplots/",sitename,"_smth.pdf", sep=''))
  strat.plot(outmat*100, yvar=newyr, y.rev=TRUE, x.names=taxanames, 
             scale.percent=FALSE, cex.xlabel=0.7, cex.yaxis=0.7)
  dev.off()

  #saving a new csv file with the smoothed data for each site
  #These smoothed csv files are saved in the smthdata folder
  outmat <- cbind(newyr, outmat)
  rownames(outmat) <- paste(site, seq(1:length(newyr)), sep='')
  colnames(outmat) <- c("YrBP", taxanames)
  write.csv(outmat, paste("./smthdata/",sitename,"_smth.csv", sep=''))

}
```

### Creating and Saving Composite dataframes
This code is used to compile all of the site csv files into one large csv. We used this composite csv for the rest of our analysis. The composite csv is saved within the composite folder.
```{r Creating_composite_record, echo=TRUE}

nsites = i #retrieve n number of sites from an earlier loop

#Make a combined file that includes all of the 29 sites in one csv

bigmat = NULL #create blank variable

files = list.files("./smthdata/", glob2rx("*.csv")) #import all the smoothed sites from the smthdata folder

for (i in 1:length(files)) {
  poll = read.csv(paste("./smthdata/",files[i],sep='')) #read in site one by one
  if (i == 1) {
    bigmat = poll #for the first site, bigmat is the entire data frame
  } else {
    bigmat = rbind.fill(bigmat,poll) #all other sites are added to the initial dataframe through a row bind, adding each site beneath the next
  }
}

colnames(bigmat)[1] <- "Sample" #rename column 1 to Sample
write.csv(bigmat,paste("./composite/NAAll_",halfWin,".csv",sep=''),
          row.names=FALSE) #save the composite file of all the smoothed sites to the composite folder


#repeat the same code but this time with the raw data to create a composite csv of the original site raw data and saving the raw data composite file to the composite folder in the working directory
bigmat = NULL

files = list.files("./rawdata/", glob2rx("*.csv"))

for (i in 1:length(files)) {
  poll = read.csv(paste("./rawdata/",files[i],sep=''))
  if (i == 1) {
    bigmat = poll
  } else {
    bigmat = rbind.fill(bigmat,poll)
  }
}

colnames(bigmat)[2] <- "YrBP"
write.csv(bigmat,"./composite/NAAll_raw.csv",
          row.names=FALSE)
```
